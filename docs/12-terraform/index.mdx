# Terraform

This page provides an introduction to Terraform. These are my notes from the Terraform Associate Certificate.

## Overview

Terraform is an infrastructure as code tool that lets you define both cloud and on-prem resources in human-readable configuration files that you can version, reuse, and share.

Terraform can manage low-level components like compute, storage, and networking resources, as well as high-level components like DNS entries and SaaS features.

You can find all publicly available providers on the [Terraform Registry](https://registry.terraform.io/), including Amazon Web Services (AWS), Azure, Google Cloud Platform (GCP), Kubernetes, Helm, GitHub, Splunk, DataDog, and many more.

Terraform workflow consists of three stages,

- Write: You define resources, which may be across multiple cloud providers and services. For example, you might create a configuration to deploy an application on virtual machines in a Virtual Private Cloud (VPC) network with security groups and a load balancer.

- Plan: Terraform creates an execution plan describing the infrastructure it will create, update, or destroy based on the existing infrastructure and your configuration.

- Apply: On approval, Terraform performs the proposed operations in the correct order, respecting any resource dependencies. For example, if you update the properties of a VPC and change the number of virtual machines in that VPC, Terraform will recreate the VPC before scaling the virtual machines.

Terraform providers automatically calculate dependencies between resources to create or destroy them in the correct order.

## Terraform Building Blocks

Terraform is logically split into two main parts Terraform Core and Terraform Plugins. 

### Terraform Core

Terraform Core is a statically-compiled binary written in the Go programming language. The primary responsibilities of Terraform Core are,

- Reading and interpolating configuration files and modules
- Resource state management
- Construction of the Resource Graph
- Plan execution
- Communication with plugins over RPC

### Terraform Plugins

Terraform Plugins are written in Go and are executable binaries invoked by Terraform Core over RPC. Each plugin exposes an implementation for a specific service, such as AWS, or provisioner, such as bash. The primary responsibilities of Provider Plugins are, 

- Initialization of any included libraries used to make API calls
- Authentication with the Infrastructure Provider
- Define managed resources and data sources that map to specific services
- Define functions that enable or simplify computational logic for practitioner configurations
- Executing commands or scripts on the designated Resource after creation, or on destruction.


When terraform init is run, Terraform reads configuration files in the working directory to determine which plugins are necessary, searches for installed plugins in several locations, sometimes downloads additional plugins, decides which plugin versions to use, and writes a lock file to ensure Terraform will use the same plugin versions in this directory until terraform init runs again.

## Terraform Language Constructs

Terraform language contains below major constructs, 

### Terraform Block

The terraform {} block contains terraform settings, including the required providers Terraform will use to provision your infrastructure.

### Providers

The provider block configures the specified provider, in this case aws. A provider is a plugin that Terraform uses to create and manage your resources. You can even use different providers together. [Complete list of providers supported by hashicorp can be found here.](https://registry.terraform.io/search/providers?namespace=hashicorp)

Each provider adds a set of resource types and/or data sources that Terraform can manage. Terraform CLI finds and installs providers when initializing a working directory.

#### Provider Configuration

Provider configurations belong in the root module of a Terraform configuration.(Child modules receive their provider configurations from the root module)

A provider configuration is created using a provider block, 

```hcl
provider "google" {
  project = "acme-app"
  region  = "us-central1"
}
```

You can use expressions in the values of these configuration arguments, but can only reference values that are known before the configuration is applied. This means you can safely reference input variables, but not attributes exported by resources.

You can optionally define multiple configurations for the same provider, and select which one to use on a per-resource or per-module basis. The primary reason for this is to support multiple regions for a cloud platform

To create multiple configurations for a given provider, include multiple provider blocks with the same provider name. For each additional non-default configuration, use the alias meta-argument to provide an extra name segment. For example,

```hcl
# The default provider configuration; resources that begin with `aws_` will use
# it as the default, and it can be referenced as `aws`.
provider "aws" {
  region = "us-east-1"
}

# Additional provider configuration for west coast region; resources can
# reference this as `aws.west`.
provider "aws" {
  alias  = "west"
  region = "us-west-2"
}
```

By default, resources use a default provider configuration (one without an alias argument) inferred from the first word of the resource type name.

To use an alternate provider configuration for a resource or data source, set its provider meta-argument to a &lt;PROVIDER NAME&gt;.&lt;ALIAS&gt; reference,

```hcl
resource "aws_instance" "foo" {
  provider = aws.west

  # ...
}
```

### Resources

Use resource blocks to define components of your infrastructure. A resource might be a physical or virtual component such as an EC2 instance, or it can be a logical resource such as a Heroku application.

Resource blocks have two strings before the block the resource type and the resource name. 

Resource blocks contain arguments which you use to configure the resource. Arguments can include things like machine sizes, disk image names, or VPC IDs. [AWS providers reference](https://registry.terraform.io/providers/hashicorp/aws/latest/docs) lists the required and optional arguments for each resource. For your EC2 instance, the example configuration sets the AMI ID to an Ubuntu image, and the instance type to t2.micro.


## Installing Terraform
```bash
brew tap hashicorp/tap
brew install hashicorp/tap/terraform
```

## Updating Terraform
```bash
brew update
brew upgrade hashicorp/tap/terraform
```

## Core Terraform Workflow

- Developer write Terraform configuration just like you write code and creates PR.
- CI/CD outputs terraform plan in the PR automatically.
- Reviewer reviews PR.
- Once PR is approved, developer merges the PR and watches how it is applied to actual infrastructure.

## Provision NGINX Server on Docker

After you install Terraform and Docker on your local machine, start Docker Desktop.
```bash
open -a Docker
```

Create a directory named learn-terraform-docker-container.
```bash
mkdir learn-terraform-docker-container
cd learn-terraform-docker-container
```

In the working directory, create a file called main.tf and paste the following Terraform configuration into it.
```hcl
terraform {
  required_providers {
    docker = {
      source  = "kreuzwerker/docker"
      version = "~> 3.0.1"
    }
  }
}

provider "docker" {}

resource "docker_image" "nginx" {
  name         = "nginx"
  keep_locally = false
}

resource "docker_container" "nginx" {
  image = docker_image.nginx.image_id
  name  = "tutorial"

  ports {
    internal = 80
    external = 8000
  }
}
```

Initialize the project, which downloads a plugin called a provider that lets Terraform interact with Docker.

```bash
terraform init
```


Provision the NGINX server container with apply. When Terraform asks you to confirm type yes and press ENTER.

```bash
terraform apply
```

To stop the container, run terraform destroy.

```bash
terraform destroy
```


## Provision EC2 Instance on AWS

To use your IAM credentials to authenticate the Terraform AWS provider, set the AWS_ACCESS_KEY_ID environment variable.

```bash
export AWS_ACCESS_KEY_ID=
export AWS_SECRET_ACCESS_KEY=
```

Each Terraform configuration must be in its own working directory. Create a directory for your configuration.

```bash
mkdir learn-terraform-aws-instance
cd learn-terraform-aws-instance
```

Create a file to define your infrastructure.

```hcl
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 4.16"
    }
  }

  required_version = ">= 1.2.0"
}

provider "aws" {
  region  = "us-west-2"
}

resource "aws_instance" "app_server" {
  ami           = "ami-830c94e3"
  instance_type = "t2.micro"

  tags = {
    Name = "ExampleAppServerInstance"
  }
}
```

Initialize the directory using terraform init

The terraform fmt command automatically updates configurations in the current directory for readability and consistency.

```bash
 terraform fmt
```

You can also make sure your configuration is syntactically valid and internally consistent by using the terraform validate command.
```bash
terraform validate
Success! The configuration is valid.
```

Apply the configuration now with the terraform apply command. 

Before it applies any changes, Terraform prints out the execution plan which describes the actions Terraform will take in order to change your infrastructure to match the configuration. The output format is similar to the diff format generated by tools such as Git.


When you applied your configuration, Terraform wrote data into a file called terraform.tfstate. Terraform stores the IDs and properties of the resources it manages in this file, so that it can update or destroy those resources going forward.

The Terraform state file is the only way Terraform can track which resources it manages, and often contains sensitive information, so you must store your state file securely and restrict access to only trusted team members who need to manage your infrastructure.

Inspect the current state using terraform show.
```bash
terraform show

aws_instance.app_server:
resource "aws_instance" "app_server" {
    ami                          = "ami-830c94e3"
    arn                          = "arn:aws:ec2:us-west-2:561656980159:instance/i-01e03375ba238b384"
    associate_public_ip_address  = true
    availability_zone            = "us-west-2c"
    cpu_core_count               = 1
    cpu_threads_per_core         = 1
    disable_api_termination      = false
    ebs_optimized                = false
    get_password_data            = false
    hibernation                  = false
    id                           = "i-01e03375ba238b384"
    instance_state               = "running"
    instance_type                = "t2.micro"
    ipv6_address_count           = 0
    ipv6_addresses               = []
    monitoring                   = false
    primary_network_interface_id = "eni-068d850de6a4321b7"
    private_dns                  = "ip-172-31-0-139.us-west-2.compute.internal"
    private_ip                   = "172.31.0.139"
    public_dns                   = "ec2-18-237-201-188.us-west-2.compute.amazonaws.com"
    public_ip                    = "18.237.201.188"
    secondary_private_ips        = []
    security_groups              = [
        "default",
    ]
    source_dest_check            = true
    subnet_id                    = "subnet-31855d6c"
    tags                         = {
        "Name" = "ExampleAppServerInstance"
    }
    tenancy                      = "default"
    vpc_security_group_ids       = [
        "sg-0edc8a5a",
    ]

    credit_specification {
        cpu_credits = "standard"
    }

    enclave_options {
        enabled = false
    }

    metadata_options {
        http_endpoint               = "enabled"
        http_put_response_hop_limit = 1
        http_tokens                 = "optional"
    }

    root_block_device {
        delete_on_termination = true
        device_name           = "/dev/sda1"
        encrypted             = false
        iops                  = 0
        tags                  = {}
        throughput            = 0
        volume_id             = "vol-031d56cc45ea4a245"
        volume_size           = 8
        volume_type           = "standard"
    }
}
```

When Terraform created this EC2 instance, it also gathered the resource's metadata from the AWS provider and wrote the metadata to the state file.


Terraform has a built-in command called terraform state for advanced state management. Use the list subcommand to list of the resources in your project's state.
```bash
terraform state list
aws_instance.app_server
```

## Changing Resources

Change the aws_instance.app_server resource under the provider block in main.tf by replacing the current AMI ID with a new one.

```hcl
 resource "aws_instance" "app_server" {
-  ami           = "ami-830c94e3"
+  ami           = "ami-08d70e59c07c61a3a"
   instance_type = "t2.micro"
 }
```

This update changes the AMI to an Ubuntu 16.04 AMI. The AWS provider knows that it cannot change the AMI of an instance after it has been created, so Terraform will destroy the old instance and create a new one.

After changing the configuration, run terraform apply again to see how Terraform will apply this change to the existing resources.


## Lock and upgrade provider versions

Terraform providers manage resources by communicating between Terraform and target APIs. Whenever the target APIs change or add functionality, provider maintainers may update and version the provider.

When multiple users or automation tools run the same Terraform configuration, they should all use the same versions of their required providers. There are two ways for you to manage provider versions in your configuration.

- Specify provider version constraints in your configuration's terraform block.
- Use the dependency lock file

Let's create a S3 bucket from an initialized Terraform configuration and then will update the Terraform dependency lock file to use the latest version of the AWS provider, and edit the Terraform configuration to conform to the new provider version's requirements.

Clone the Learn Terraform Provider Versioning repository.

```bash
git clone https://github.com/hashicorp/learn-terraform-provider-versioning.git
```

Navigate to the repository directory in your terminal.

```bash
cd learn-terraform-provider-versioning
```

This directory is a pre-initialized Terraform project with three files: main.tf, terraform.tf, and .terraform.lock.hcl. HashiCorp has released a newer version of the AWS provider since this workspace was first initialized.

Open the terraform.tf file. Here you will find the terraform block which specifies the required provider version and required Terraform version for this configuration.

```hcl
terraform {
  required_providers {
    random = {
      source  = "hashicorp/random"
      version = "3.1.0"
    }

    aws = {
      source  = "hashicorp/aws"
      version = ">= 4.5.0"
    }
  }

  required_version = "~> 1.2"
}
```

The terraform block contains the required_providers block, which specifies the provider local name, the source address, and the version.

When you initialize a Terraform configuration for the first time with Terraform 1.1 or later, Terraform will generate a new .terraform.lock.hcl file in the current working directory. You should include the lock file in your version control repository to ensure that Terraform uses the same provider versions across your team and in ephemeral remote execution environments.

Open the .terraform.lock.hcl file.

```hcl
# This file is maintained automatically by "terraform init".
# Manual edits may be lost in future updates.

provider "registry.terraform.io/hashicorp/aws" {
  version     = "4.5.0"
  constraints = ">= 4.5.0"
  hashes = [
    "h1:PR5m6lcJZzSIYqfhnMd0YWTN+On2XGgfYV5AKIvOvBo=",
    "zh:0573de96ba316d808be9f8d6fc8e8e68e0e6b614ed4d707bd236c4f7b46ac8b1",
## ...
    "zh:f4722596e8b5f012013f87bf4d2b7d302c248a04a144de4563b3e3f754a30c51",
  ]
}

provider "registry.terraform.io/hashicorp/random" {
  version     = "3.1.0"
  constraints = "3.1.0"
  hashes = [
    "h1:9cCiLO/Cqr6IUvMDSApCkQItooiYNatZpEXmcu0nnng=",
    "zh:2bbb3339f0643b5daa07480ef4397bd23a79963cc364cdfbb4e86354cb7725bc",
## ...
    "zh:d9e13427a7d011dbd654e591b0337e6074eef8c3b9bb11b2e39eaaf257044fd7",
    "zh:f7605bd1437752114baf601bdf6931debe6dc6bfe3006eb7e9bb9080931dca8a",
  ]
}
```

Notice the two providers specified in your terraform.tf file. The AWS provider version is v4.5.0. This fulfills the >=4.5.0 constraint, but is no longer the latest version of the AWS provider. The random provider is set to v3.1.0 and fulfills its version constraints.

Initialize this configuration.

```bash
terraform init

Initializing the backend...

Initializing provider plugins...
- Reusing previous version of hashicorp/aws from the dependency lock file
- Reusing previous version of hashicorp/random from the dependency lock file
- Installing hashicorp/aws v4.5.0...
- Installed hashicorp/aws v4.5.0 (signed by HashiCorp)
- Installing hashicorp/random v3.1.0...
- Installed hashicorp/random v3.1.0 (signed by HashiCorp)

Terraform has been successfully initialized!
```

Notice that instead of installing the latest version of the AWS provider that conforms with the configured version constraints, Terraform installed the version specified in the lock file. While initializing your workspace, Terraform read the dependency lock file and downloaded the specified versions of the AWS and random providers.

The following table shows which provider Terraform would download in this scenario, based on the version constraint and presence of a lock file.


|Provider	| Version Constraint |terraform init (no lock file)|terraform init (lock file) |
|---------|--------------------|-----------------------------|---------------------------|
|aws		  | >= 4.5.0	         |Latest version (e.g. 5.55.0) |Lock file version (4.5.0)  |
|random	  | 3.1.0	             |3.1.0												 |Lock file version (3.1.0)  |


:::info
If Terraform did not find a lock file, it would download the latest versions of the providers that fulfill the version constraints you defined in the required_providers block.
:::


The -upgrade flag will upgrade all providers to the latest version consistent within the version constraints specified in your configuration.

```bash
terraform init -upgrade

Initializing HCP Terraform...

Initializing provider plugins...
- Finding hashicorp/aws versions matching ">= 4.5.0"...
- Finding hashicorp/random versions matching "3.1.0"...
- Installing hashicorp/aws v5.56.1...
- Installed hashicorp/aws v5.56.1 (signed by HashiCorp)
- Using previously-installed hashicorp/random v3.1.0

Terraform has made some changes to the provider dependency selections recorded
in the .terraform.lock.hcl file. Review those changes and commit them to your
version control system if they represent changes you intended to make.

Terraform has been successfully initialized!
```

:::info
Notice that Terraform installs the latest version of the AWS provider.
:::

Open the .terraform.lock.hcl file and notice that the AWS provider's version is now the latest version.

```hcl
provider "registry.terraform.io/hashicorp/aws" {
  version     = "5.56.1"
  constraints = ">= 4.5.0"
  ## ...
}
```

:::info
You can also use the -upgrade flag to downgrade the provider versions if the version constraints are modified to specify a lower provider version that the one specified in the lock file.
:::

Plan your configuration using terraform plan to ensure that it works with the upgraded provider. If the plan step completes successfully, it is safe to commit the configuration with the updated lock file to version control. If the plan or apply steps fail, do not commit the lock file to version control until you've resolved the error.


## Terraform Variables

Terraform variables allow you to write configuration that is flexible and easier to re-use. Let's add a variable to define the instance name.

Create a new file called variables.tf with a block defining a new instance_name variable.

```hcl
variable "instance_name" {
  description = "Value of the Name tag for the EC2 instance"
  type        = string
  default     = "ExampleAppServerInstance"
}
```

:::info
Terraform loads all files in the current directory ending in .tf, so you can name your configuration files however you choose.
:::

In main.tf, update the aws_instance resource block to use the new variable. The instance_name variable block will default to its default value ("ExampleAppServerInstance") unless you declare a different value.

```hcl
 resource "aws_instance" "app_server" {
   ami           = "ami-08d70e59c07c61a3a"
   instance_type = "t2.micro"

   tags = {
-    Name = "ExampleAppServerInstance"
+    Name = var.instance_name
   }
 }
```

Apply the configuration using terraform apply command. Respond to the confirmation prompt with a yes.

Now apply the configuration again, this time overriding the default instance name by passing in a variable using the -var flag. Terraform will update the instance's Name tag with the new name. Respond to the confirmation prompt with yes.

Setting variables via the command-line will not save their values. Terraform supports many ways to use and set variables so you can avoid having to enter them repeatedly as you execute commands. 


## Terraform Output

Use output to present useful information to the Terraform user.

Create a file called outputs.tf in your learn-terraform-aws-instance directory. Add the configuration below to outputs.tf to define outputs for your EC2 instance's ID and IP address.

```hcl
output "instance_id" {
  description = "ID of the EC2 instance"
  value       = aws_instance.app_server.id
}

output "instance_public_ip" {
  description = "Public IP address of the EC2 instance"
  value       = aws_instance.app_server.public_ip
}
```

Apply your configuration now. Respond to the confirmation prompt with yes.

```bash
Outputs:

instance_id = "i-0bf954919ed765de1"
instance_public_ip = "54.186.202.254"
```

Terraform prints output values to the screen when you apply your configuration. Query the outputs with the terraform output command.


```bash
terraform output
instance_id = "i-0bf954919ed765de1"
instance_public_ip = "54.186.202.254"
```

You can use Terraform outputs to connect your Terraform projects with other parts of your infrastructure, or with other Terraform projects.


## Terraform State

State is a necessary requirement for Terraform to function. Terraform requires some sort of database to map Terraform config to the real world. For example, when you have a resource resource "aws_instance" "foo" in your configuration, Terraform uses this mapping to know that the resource resource "aws_instance" "foo" represents a real world object with the instance ID i-abcd1234 on a remote system.

For some providers like AWS, Terraform could theoretically use something like AWS tags. Early prototypes of Terraform actually had no state files and used this method. However, we quickly ran into problems. The first major issue was a simple one: not all resources support tags, and not all cloud providers support tags.

Therefore, for mapping configuration to resources in the real world, Terraform uses its own state structure.

Alongside the mappings between resources and remote objects, Terraform must also track metadata such as resource dependencies.  Terraform could know that servers must be deleted before the subnets they are a part of. The complexity for this approach quickly explodes, however in addition to Terraform having to understand the ordering semantics of every resource for every provider, Terraform must also understand the ordering across providers.

In addition to basic mapping, Terraform stores a cache of the attribute values for all resources in the state. This is the most optional feature of Terraform state and is done only as a performance improvement.

For small infrastructures, Terraform can query your providers and sync the latest attributes from all your resources.For larger infrastructures, querying every resource is too slow. Many cloud providers do not provide APIs to query multiple resources at once, and the round trip time for each resource is hundreds of milliseconds.

In the default configuration, Terraform stores the state in a file in the current working directory where Terraform was run. This is okay for getting started, but when using Terraform in a team it is important for everyone to be working with the same state so that operations will be applied to the same remote objects. Terraform can use remote locking as a measure to avoid two or more different users accidentally running Terraform at the same time, and thus ensure that each Terraform run begins with the most recent updated state.

## Store Remote state

In production environments you should keep your state secure and encrypted, where your teammates can access it to collaborate on infrastructure.

TODO: Write Details from https://developer.hashicorp.com/terraform/language/backend/s3
